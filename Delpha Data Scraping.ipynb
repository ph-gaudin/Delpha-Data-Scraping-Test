{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c0142f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T09:04:56.393508Z",
     "start_time": "2022-12-03T09:04:54.441277Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc073a",
   "metadata": {},
   "source": [
    "Source used to find most data was Wikipedia.\n",
    "\n",
    "The exception are the social media links, which I found by scraping the front pages of each individual company website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c1a0689",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-03T09:05:25.510919Z",
     "start_time": "2022-12-03T09:05:22.085668Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Headquarters</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Number of employees</th>\n",
       "      <th>Website</th>\n",
       "      <th>Social Media Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>One Microsoft Way Redmond, Washington, U.S.</td>\n",
       "      <td>1.983000e+11</td>\n",
       "      <td>221000</td>\n",
       "      <td>microsoft.com</td>\n",
       "      <td>[https://www.facebook.com/Microsoft, https://t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HSBC</td>\n",
       "      <td>8 Canada Square London, England, UK</td>\n",
       "      <td>4.955200e+10</td>\n",
       "      <td>219697</td>\n",
       "      <td>www.hsbc.com</td>\n",
       "      <td>[https://twitter.com/HSBC, https://www.linkedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BNP Paribas</td>\n",
       "      <td>Boulevard des Italiens, Paris, France</td>\n",
       "      <td>4.620000e+10</td>\n",
       "      <td>190000</td>\n",
       "      <td>group.bnpparibas</td>\n",
       "      <td>[https://twitter.com/BNPParibas, https://www.l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dataiku</td>\n",
       "      <td>New York City, United States</td>\n",
       "      <td>1.500000e+08</td>\n",
       "      <td>1000</td>\n",
       "      <td>dataiku.com</td>\n",
       "      <td>[https://www.facebook.com/dataiku/, https://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Salesforce</td>\n",
       "      <td>Salesforce Tower San Francisco, California, U.S.</td>\n",
       "      <td>2.649000e+10</td>\n",
       "      <td>73542</td>\n",
       "      <td>salesforce.com</td>\n",
       "      <td>[http://www.facebook.com/salesforceFrance, htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bouygues Construction</td>\n",
       "      <td>Challenger à Guyancourt ( Saint-Quentin-en-Yve...</td>\n",
       "      <td>1.280000e+10</td>\n",
       "      <td>52800</td>\n",
       "      <td>bouygues-construction.com</td>\n",
       "      <td>[https://www.bouygues-construction.com/blog/fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name                                       Headquarters  \\\n",
       "0              Microsoft        One Microsoft Way Redmond, Washington, U.S.   \n",
       "1                   HSBC                8 Canada Square London, England, UK   \n",
       "2            BNP Paribas              Boulevard des Italiens, Paris, France   \n",
       "3                Dataiku                       New York City, United States   \n",
       "4             Salesforce   Salesforce Tower San Francisco, California, U.S.   \n",
       "5  Bouygues Construction  Challenger à Guyancourt ( Saint-Quentin-en-Yve...   \n",
       "\n",
       "        Revenue  Number of employees                     Website  \\\n",
       "0  1.983000e+11               221000               microsoft.com   \n",
       "1  4.955200e+10               219697                www.hsbc.com   \n",
       "2  4.620000e+10               190000            group.bnpparibas   \n",
       "3  1.500000e+08                 1000                 dataiku.com   \n",
       "4  2.649000e+10                73542              salesforce.com   \n",
       "5  1.280000e+10                52800  bouygues-construction.com    \n",
       "\n",
       "                                  Social Media Links  \n",
       "0  [https://www.facebook.com/Microsoft, https://t...  \n",
       "1  [https://twitter.com/HSBC, https://www.linkedi...  \n",
       "2  [https://twitter.com/BNPParibas, https://www.l...  \n",
       "3  [https://www.facebook.com/dataiku/, https://ww...  \n",
       "4  [http://www.facebook.com/salesforceFrance, htt...  \n",
       "5  [https://www.bouygues-construction.com/blog/fr...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Aggregating the wikipedia pages of 5 of the companies, Bouygues only has a French wikipedia page, so is added later\n",
    "pages=['https://en.wikipedia.org/wiki/Microsoft',\n",
    "       'https://en.wikipedia.org/wiki/HSBC',\n",
    "       'https://en.wikipedia.org/wiki/BNP_Paribas',\n",
    "       'https://en.wikipedia.org/wiki/Dataiku',\n",
    "       'https://en.wikipedia.org/wiki/Salesforce']\n",
    "\n",
    "\n",
    "#Empty dictionary to gradually fill with data\n",
    "data={'Name':[],'Headquarters':[],'Revenue':[],'Number of employees':[],'Website':[],'Social Media Links':[]}\n",
    "\n",
    "\n",
    "\n",
    "#Scraping each English wikipedia page\n",
    "for n in pages:\n",
    "    page = requests.get(n)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    data['Name'].append(soup.find('h1').get_text())  #Names are found in page headers\n",
    "    table=soup.find('table',{'class':'infobox vcard'})  #Most data is extracted from the infobox on the side of the page\n",
    "    result = {}\n",
    "    \n",
    "    for tr in table.find_all('tr'): \n",
    "        if tr.find('th'):\n",
    "            result[tr.find('th').text] = \\\n",
    "            tr.find('td').get_text(separator=\" \").replace('  ',' ').replace(' ,',',').replace('\\xa0','').replace(' .','.')\n",
    "    #Parsing through the infobox data and cleaning it to make more readable\n",
    "            \n",
    "    data['Headquarters'].append(result['Headquarters'])\n",
    "    data['Revenue'].append(result['Revenue'])\n",
    "    data['Number of employees'].append(result['Number of employees'])\n",
    "    data['Website'].append(result['Website'])\n",
    "    \n",
    "\n",
    "    \n",
    "#Same process as above for Bouygues, except changed to fit the specifics of French Wikipedia\n",
    "page = requests.get('https://fr.wikipedia.org/wiki/Bouygues_Construction')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "data['Name'].append(soup.find('h1').get_text())\n",
    "table=soup.find('table',{'class':'infobox_v2 noarchive'})\n",
    "result = {}\n",
    "\n",
    "for tr in table.find_all('tr'):\n",
    "    if tr.find('th'):\n",
    "        result[tr.find('th').text] = \\\n",
    "        tr.find('td').get_text(separator=\" \").replace('  ',' ').replace(' ,',',').replace('\\xa0','').replace('\\n','')\n",
    "        \n",
    "data['Headquarters'].append(result['Siège social\\n'])\n",
    "data['Revenue'].append(result[\"Chiffre d'affaires\\n\"])\n",
    "data['Number of employees'].append(result['Effectif\\n'])\n",
    "data['Website'].append(result['Site web\\n'])    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Further cleaning Revenue data, in order to have a float at the end\n",
    "temp=[]\n",
    "for x in data['Revenue']:\n",
    "    if 'billion' in x:\n",
    "        temp.append(float(x.split('billion')[0].split()[1])*1000000000)\n",
    "    elif 'million' in x:\n",
    "        temp.append(float(x.split('million')[0].split()[1])*1000000)\n",
    "    elif 'Mds' in x:\n",
    "        temp.append(float(x.split()[0].replace(',','.'))*1000000000)\n",
    "\n",
    "data['Revenue']=temp\n",
    "\n",
    "\n",
    "#Further cleaning Employee data, in order to have an int at the end\n",
    "temp=[]\n",
    "for x in data['Number of employees']:\n",
    "    if 'en' in x:\n",
    "        temp.append(int(x.split('en')[0].replace(' ','')))\n",
    "    else:\n",
    "        temp.append(int(x.split()[0].replace('+','').replace(',','')))\n",
    "data['Number of employees']=temp\n",
    "\n",
    "\n",
    "\n",
    "#Individually scraping through the front webpages of each company, in order to find social media links\n",
    "#usually located at the bottom of the page, with small changes to accomodate each page\n",
    "#Links are then added as a list to their respective dictionary entries\n",
    "temp=[]\n",
    "page = requests.get('https://www.microsoft.com/en-us/')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "for x in soup.find_all('li',{'class':\"list-inline-item mr-g\"}):\n",
    "    temp.append(x.find('a')['href'])\n",
    "data['Social Media Links'].append(temp)\n",
    "\n",
    "temp=[]\n",
    "page = requests.get('https://www.hsbc.com/')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "for x in soup.find_all('li',{'class':\"footer-social__item\"}):\n",
    "    temp.append(x.find('a')['href'])\n",
    "data['Social Media Links'].append(temp)\n",
    " \n",
    "temp=[]\n",
    "page = requests.get('https://group.bnpparibas/en/')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup = soup.find('div',{'class','col-md-2 col-sm-3 col-xs-6 footer-social links'})\n",
    "for x in soup.find_all('li'):\n",
    "     temp.append(x.find('a')['href'])\n",
    "data['Social Media Links'].append(temp)\n",
    "    \n",
    "temp=[]\n",
    "page = requests.get('https://www.dataiku.com/')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup = soup.find('ul',{'class','socials'})\n",
    "for x in soup.find_all('li'):\n",
    "    temp.append(x.find('a')['href'])\n",
    "data['Social Media Links'].append(temp)\n",
    "\n",
    "temp=[]\n",
    "page = requests.get('https://www.salesforce.com/fr/?ir=1')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup = soup.find('div',{'class','footer-social-links'})\n",
    "for x in soup.find_all('a'):\n",
    "    temp.append(x['href'])\n",
    "data['Social Media Links'].append(temp)\n",
    "\n",
    "temp=[]\n",
    "page = requests.get('https://www.bouygues-construction.com/')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup = soup.find('div',{'class','block block-bouygues-main no-title even last block-count-10 block-region-footer block-top-social-links'})\n",
    "for x in soup.find_all('li'):\n",
    "    temp.append(x.find('a')['href'])\n",
    "data['Social Media Links'].append(temp)\n",
    "\n",
    "\n",
    "#Final dataframe\n",
    "df=pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea689655",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T18:46:26.697914Z",
     "start_time": "2022-12-02T18:46:26.543599Z"
    }
   },
   "source": [
    "# Data visualization done in Tableau can be found here:\n",
    "\n",
    "https://public.tableau.com/app/profile/philippe.gaudin/viz/DelphaDataScrapingTest/CompanyDashboard?publish=yes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
